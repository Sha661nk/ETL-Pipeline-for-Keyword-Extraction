{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "50ab83f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Shashank\n",
      "[nltk_data]     Gupta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Shashank\n",
      "[nltk_data]     Gupta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import yake \n",
    "import time\n",
    "import pandas as pd\n",
    "from summa import keywords as sk\n",
    "from keybert import KeyBERT\n",
    "from googlesearch import search\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "  \n",
    "kw_model = KeyBERT('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "stop_words = set(stopwords.words('english') + stop_word)\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "ps = PorterStemmer()\n",
    "\n",
    "tag_re = re.compile(r'<[^>]+>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d8e33b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapper(query_string, num_of_pages, stop_at):\n",
    "    \"\"\"Scrapes search results for the provided query string.\"\"\"\n",
    "    documents = []\n",
    "    for search_result in search(query_string, tld=\"com\", num=num_of_pages, stop=stop_at, pause=2):\n",
    "        try:\n",
    "            html_content = urlopen(search_result).read()\n",
    "            soup = BeautifulSoup(html_content, features=\"html.parser\")\n",
    "            documents.append(soup.body)\n",
    "        except:\n",
    "            pass\n",
    "    return documents\n",
    "\n",
    "def cleaner(text):\n",
    "    \"\"\"Cleans the provided text by removing tags, newlines, emails, symbols, etc.\"\"\"\n",
    "    text = tag_re.sub('', str(text))\n",
    "    text = text.replace(r'\\n', ' ').replace('\\n', '')\n",
    "    text = re.sub(r'[A-Za-z0-9]*@[A-Za-z]*\\.?[A-Za-z0-9]*', ' ', text)\n",
    "    text = re.sub('[^A-Za-z0-9]+', ' ', text)\n",
    "    text = re.sub(r\"(^|\\W)\\d+\", ' ', text)\n",
    "    text = lemmatizer.lemmatize(text)\n",
    "    text = ps.stem(text)\n",
    "    return text\n",
    "\n",
    "def keyword_extractor(method, corpus, top_n, max_ngram_size):\n",
    "    \"\"\"Extracts keywords from the corpus using the specified method.\"\"\"\n",
    "    try:\n",
    "        if method == 'keybert':\n",
    "            return [word[0] for word in kw_model.extract_keywords(corpus, keyphrase_ngram_range=(1, max_ngram_size), stop_words='english', top_n=top_n)]\n",
    "\n",
    "        if method == 'text_rank':\n",
    "            return [word[0] for word in sk.keywords(corpus, scores=True)[:top_n]]\n",
    "\n",
    "        if method == 'yake':\n",
    "            kw_yake = yake.KeywordExtractor(top=top_n, stopwords=stop_words, n=max_ngram_size)\n",
    "            return [word[0] for word in kw_yake.extract_keywords(corpus)]\n",
    "\n",
    "    except:\n",
    "        print('Error: Keyword extraction method not found!')\n",
    "        return []\n",
    "\n",
    "def driver(query_string, num_of_pages, stop_at, top_n_words, grams, method):\n",
    "    \"\"\"Main driver function to extract and aggregate keywords.\"\"\"\n",
    "    data = []\n",
    "    for query in query_string:\n",
    "        documents = scrapper(query, num_of_pages, stop_at)\n",
    "        keywords = [word for doc in documents for word in keyword_extractor(method, cleaner(doc), top_n_words, grams)]\n",
    "        keywords = [k for k in keywords if set(k.split(' ')).isdisjoint(stop_words)]\n",
    "        \n",
    "        keyword_counts = [[keyword, keywords.count(keyword)] for keyword in set(keywords)]\n",
    "        keyword_freq_df = pd.DataFrame(keyword_counts, columns=['Keyword', 'Freq']).sort_values(by='Freq', ascending=False).reset_index(drop=True)\n",
    "        keyword_freq_df = keyword_freq_df[keyword_freq_df[\"Freq\"] > 3]\n",
    "        keyword_freq_df['Google Search Query'] = query\n",
    "        \n",
    "        data.append(keyword_freq_df)\n",
    "\n",
    "    final_keywords_df = pd.concat(data)\n",
    "    final_keywords_df.to_json('keywords.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f9c26622",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 55.909438610076904 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "driver(query_string = ['Data Mining', 'Machine Learning'], num_of_pages = 10, stop_at = 10, top_n_words = 100, grams = 3, method = 'yake')\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5d2f0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#keyword_extractor('keybert', clean[0], 10)\n",
    "#keyword_extractor('text_rank', clean[doc], 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
